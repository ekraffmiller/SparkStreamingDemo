/*
 * To change this license header, choose License Headers in Project Properties.
 * To change this template file, choose Tools | Templates
 * and open the template in the editor.
 */
package edu.harvard.iq.sparkstreamingml;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.types.StructType;
    
/**
 * Reads the Kafka Queue generated by HashTags.java, and
 * aggregates the hash tag counts by time window using structured streaming.
 * @author ellenk
 */
public class TwitterTrends {

    public static void main(String args[]) {
        SparkSession spark = SparkSession
                .builder()
                .appName("Twitter Trend Example")
                .master("local[4]")
                .getOrCreate();
        
    
        // Create DataSet representing the stream of input lines from kafka
        Dataset<String> lines = spark
                .readStream()
                .format("kafka")
                .option("kafka.bootstrap.servers", "localhost:9092")
                .option("subscribe", "trendsDemo")
                .option("startingOffsets", "earliest") 
                .load()
                .selectExpr("CAST(value AS STRING)")
                .as(Encoders.STRING());
               
        Dataset<EnhancedTweetRecord> records = lines.map((String s) -> {return new EnhancedTweetRecord(s);}, Encoders.bean(EnhancedTweetRecord.class));


        // Generate running word count
       // Dataset<Row> wordCounts = lines.flatMap(
       //          
       //         (FlatMapFunction<String, String>) x -> Arrays.asList(x.split(",")).iterator(),
       //         Encoders.STRING()).groupBy("value").count();

        // Start running the query that prints the running counts to the console
        StreamingQuery query = records.writeStream()
                .outputMode("append")
                .format("console")
                .start();
        /*
        Every streaming source is assumed to have offsets
        (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream. 
        The engine uses checkpointing and write ahead logs to record the offset range of the data being processed in each trigger
        */
      //  StreamingQuery fileQuery
      //  = lines.writeStream()
      //          .format("parquet") // can be "orc", "json", "csv", etc.
      //          .option("path", "/tmp/demo")
      //          .option("checkpointLocation", "/tmp/democheckpoint")
      //          .outputMode("complete")
      //          .start();
        try {
            query.awaitTermination();
        } catch (StreamingQueryException e) {
            System.out.println(e);
        }
    }
}
