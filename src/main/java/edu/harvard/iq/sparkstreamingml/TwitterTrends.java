/*
 * To change this license header, choose License Headers in Project Properties.
 * To change this template file, choose Tools | Templates
 * and open the template in the editor.
 */
package edu.harvard.iq.sparkstreamingml;

import java.sql.Timestamp;
import java.util.ArrayList;
import java.util.List;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import static org.apache.spark.sql.functions.avg;
import static org.apache.spark.sql.functions.col;
import static org.apache.spark.sql.functions.count;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import scala.Tuple3;
    
/**
 * Reads the Kafka Queue generated by HashTags.java, and
 * aggregates the hash tag counts by time window using structured streaming.
 * @author ellenk
 */
public class TwitterTrends {

    public static void main(String args[]) {
        SparkSession spark = SparkSession
                .builder()
                .appName("Twitter Trend Example")
                .master("local[4]")
                .getOrCreate();
      
    
        // Create DataSet representing the stream of input lines from kafka
        Dataset<String> lines = spark
                .readStream()
                .format("kafka")
                .option("kafka.bootstrap.servers", "localhost:9092")
                .option("subscribe", "trendsDemo2")
                .option("startingOffsets", "earliest") 
                .load()
                .selectExpr("CAST(value AS STRING)")
                .as(Encoders.STRING());
               
        Dataset<EnhancedTweetRecord> records = lines.map((String s) -> {return new EnhancedTweetRecord(s);}, Encoders.bean(EnhancedTweetRecord.class));
        
        // flatMap() to HashTags with corresponding timestamp of Tweet
        Dataset<Row> hashTags = records.flatMap(record -> { 
            List<Tuple3<String, Double, Timestamp>> result = new ArrayList<>();
          for (String word : record.getStatus().split(" ")) {
            if (word.startsWith("#")) {  
                result.add(new Tuple3<>(word,record.getPrediction(),record.getCreatedAt()));
            }
          }
          return result.iterator();
        },
        Encoders.tuple(Encoders.STRING(), Encoders.DOUBLE(),Encoders.TIMESTAMP())
      ).toDF("word", "prediction","timestamp");
        
        // Group the data by window and word and compute the count of each group
        Dataset<Row> windowedCounts = hashTags
                .withWatermark("timestamp", "10 minutes")
                .groupBy(
                        functions.window(col("timestamp"), "10 minute", "5 minute"),
                        col("word")
                ).agg(count(col("word")).alias("count"),
                      avg(col("prediction")).alias("avg"));
        
        // Filter the results to show only rows where count > 3, 
        // and order by window & count
        Dataset<Row> filtered = windowedCounts.filter(col("count").gt(3)).sort(col("window"),col("count").desc()).select("window.start", "window.end", "word","count","avg");
    

        // Start running the query that prints the running counts to the console
        StreamingQuery query = filtered.writeStream()
                .outputMode("complete")
                .format("console")
                .option("numRows", 100)
                .start();
   
        try {
            query.awaitTermination();
        } catch (StreamingQueryException e) {
            System.out.println(e);
        }
    }
}
