
package edu.harvard.iq.sparkstreamingml;

import java.sql.Timestamp;
import java.util.ArrayList;
import java.util.List;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import static org.apache.spark.sql.functions.avg;
import static org.apache.spark.sql.functions.col;
import static org.apache.spark.sql.functions.count;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import scala.Tuple3;
    
/**
 * Reads the Kafka Queue generated by HashTags.java, and
 * aggregates the hash tag counts by time window using Structured Streaming.
 * @author ellenk
 */
public class TwitterTrends {

    public static void main(String args[]) throws StreamingQueryException {
        // Turn off logging so it's easier to see console output
        Logger.getLogger("org.apache").setLevel(Level.OFF);
        
        SparkSession spark = SparkSession
                .builder()
                .appName("Twitter Trend Example")
                .master("local[6]")
                .getOrCreate();
      
    
        // Create DataSet representing the stream of input lines from kafka
        Dataset<String> lines = spark
                .readStream()
                .format("kafka")
                .option("kafka.bootstrap.servers", "localhost:9092")
                .option("subscribe", "trendsDemo2")
                .option("startingOffsets", "earliest") 
                .load()
                .selectExpr("CAST(value AS STRING)")
                .as(Encoders.STRING());
               
        Dataset<EnhancedTweetRecord> records = lines.map((String s) -> {return new EnhancedTweetRecord(s);}, Encoders.bean(EnhancedTweetRecord.class));
        
        // flatMap() to HashTags with corresponding timestamp of Tweet
        Dataset<Row> hashTags = records.flatMap(record -> { 
            List<Tuple3<String, Double, Timestamp>> result = new ArrayList<>();
          for (String word : record.getStatus().split(" ")) {
            if (word.startsWith("#")) {  
                result.add(new Tuple3<>(word,record.getPrediction(),record.getCreatedAt()));
            }
          }
          return result.iterator();
        },
        Encoders.tuple(Encoders.STRING(), Encoders.DOUBLE(),Encoders.TIMESTAMP())
      ).toDF("hashtag", "prediction","timestamp");
        
        // Group the data by window and word, and compute the count of each group
        // and the average prediction
        Dataset<Row> windowedCounts = hashTags
                .withWatermark("timestamp", "10 minutes")
                .groupBy(
                        functions.window(col("timestamp"), "10 minute", "5 minute"),
                        col("hashtag")
                ).agg(count(col("hashtag")).alias("count"),
                      avg(col("prediction")).alias("avg"));
        
        // Filter the results to show only rows where count > 3, 
        // and order by window & count
        Dataset<Row> filtered = windowedCounts.filter(col("count").gt(3)).sort(col("window"),col("count").desc()).select("window.start", "window.end", "hashtag","count","avg");
       
        
        // Start running the query that prints the running counts to the console
        StreamingQuery query = filtered.writeStream()
                .outputMode("complete")  // complete mode because we are sorting
                .format("console")
                .option("numRows", 100)
                .start();
        
               
        // Get more info about 'Streamy' tweets
        Dataset<Row> recordsDF = records.toDF("createdAt", "prediction","status");
        recordsDF.createOrReplaceTempView("records");
   
        Dataset<Row> filteredDF = spark.sql("select * from records where status LIKE('%Streamy%')");

         // Streamy tweets query
        StreamingQuery query2 = filteredDF.writeStream()
                .outputMode("append")
                .format("console")
                .option("numRows", 100)
                .option("truncate", "FALSE")
                .start();
   
      
            query2.awaitTermination();
          
            query.awaitTermination();
       
        

        
    }
}
